{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Laboratorio sobre Spark**\n",
    "\n",
    "# **Alunos:**\n",
    "- Arthur Taylor de Jesus Popov (190084642)\n",
    "- Jo√£o Lucas Pinto Vasconcelos (190089601)\n",
    "- Pablo Guilherme de J B Silva(200025791)\n",
    "- Pedro Lucas Siqueira Fernandes(190115564)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inicializa√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando a biblioteca que permite copiar conte√∫dos do Gdrive compartilhado do professor\n",
    "%pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiando a pasta de laborat√≥rio (material do professor) para o contexto do aluno\n",
    "import gdown\n",
    "url = 'https://drive.google.com/drive/folders/1z_l8RO6YYwjLdPrMBtnSNpfzfznJt1ja'\n",
    "gdown.download_folder(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defini√ß√£o de variaveis e instala√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo as vari√°veis de ambiente do Spark \n",
    "import os\n",
    "# Vari√°veis gerais\n",
    "os.environ['JAVA_HOME']=\"/usr/lib/jvm/java-11-openjdk-amd64\" # readlink -f /usr/bin/javac\n",
    "os.environ['BASHRC_PATH']= \"/root/.bashrc\"\n",
    "# Vari√°veis espec√≠ficas do Spark\n",
    "os.environ['SPARK_INSTALL_DIR']=\"/content\"\n",
    "os.environ['SPARK_HOME']=\"/content/spark\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiando os fonte do hadoop para a pasta $SPARK_INSTALL_DIR\n",
    "!wget https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz -P $SPARK_INSTALL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descompactando os arquivos do hadoop na pasta $SPARK_INSTALL_DIR\n",
    "!tar -xvzf $SPARK_INSTALL_DIR/spark-3.5.4-bin-hadoop3.tgz -C $SPARK_INSTALL_DIR\n",
    "!mv $SPARK_INSTALL_DIR/spark-3.5.4-bin-hadoop3 $SPARK_INSTALL_DIR/spark\n",
    "!rm $SPARK_INSTALL_DIR/spark-3.5.4-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ativando servidor Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando os processos NameNode e DataNode, daemons do HDFS\n",
    "!$SPARK_HOME/sbin/start-master.sh --host localhost --port 7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciando os processos relativos ao gerenciador de recursos YARN\n",
    "!$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark\n",
    "#!$SPARK_HOME/bin/pyspark --master spark://localhost:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o KAFKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando o kafka python\n",
    "%pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo os imports de Producer e Consumer do Kafka\n",
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo download do bin√°rio do kafka\n",
    "!curl -sSOL https://dlcdn.apache.org/kafka/3.8.0/kafka_2.13-3.8.0.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descompactando o kafka e criando um link para a pasta do kafka\n",
    "!tar xvfz kafka_2.13-3.8.0.tgz\n",
    "!ln -s kafka_2.13-3.8.0 kafka\n",
    "!rm kafka_2.13-3.8.0.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ativando os daemons do kafka...\n",
    "!./kafka/bin/zookeeper-server-start.sh -daemon ./kafka/config/zookeeper.properties\n",
    "!./kafka/bin/kafka-server-start.sh -daemon ./kafka/config/server.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando canais kafka\n",
    "!./kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic canalinput\n",
    "!./kafka/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic canaloutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e792335",
   "metadata": {},
   "source": [
    "## üîó Conectando √† API do Twitter\n",
    "Esta se√ß√£o configura a conex√£o com a API do Twitter usando `tweepy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1087fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tweepy\n",
    "\n",
    "import tweepy\n",
    "\n",
    "# Substitua pelas suas credenciais do Twitter\n",
    "API_KEY = \"SUA_API_KEY\"\n",
    "API_SECRET = \"SUA_API_SECRET\"\n",
    "ACCESS_TOKEN = \"SEU_ACCESS_TOKEN\"\n",
    "ACCESS_SECRET = \"SEU_ACCESS_SECRET\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# Teste de autentica√ß√£o\n",
    "user = api.verify_credentials()\n",
    "print(f\"Autenticado como: {user.screen_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb8653",
   "metadata": {},
   "source": [
    "## üì° Capturando Tweets em Tempo Real e Enviando para Kafka\n",
    "Esta se√ß√£o captura tweets sobre tecnologia e os envia para um t√≥pico no Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install confluent_kafka\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "import json\n",
    "\n",
    "producer = Producer({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "class StreamListener(tweepy.Stream):\n",
    "    def on_status(self, status):\n",
    "        if status.lang == \"pt\":  # Apenas tweets em portugu√™s\n",
    "            tweet_data = {\n",
    "                \"user\": status.user.screen_name,\n",
    "                \"text\": status.text,\n",
    "                \"created_at\": str(status.created_at)\n",
    "            }\n",
    "            producer.produce(\"tweets_stream\", json.dumps(tweet_data))\n",
    "            print(\"Tweet enviado para Kafka:\", tweet_data)\n",
    "\n",
    "listener = StreamListener(API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_SECRET)\n",
    "listener.filter(track=[\"tecnologia\", \"intelig√™ncia artificial\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233a2554",
   "metadata": {},
   "source": [
    "## üß† Processamento com IA (NLP) para Extra√ß√£o de Palavras-Chave\n",
    "Usamos `spaCy` para processar os tweets e extrair palavras relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c853eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "\n",
    "import spacy\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def process_tweet(text):\n",
    "    doc = nlp(text)\n",
    "    palavras_chave = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return \" \".join(palavras_chave)\n",
    "\n",
    "process_tweet_udf = udf(process_tweet, StringType())\n",
    "\n",
    "df = df.withColumn(\"keywords\", process_tweet_udf(df.text))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388cf32",
   "metadata": {},
   "source": [
    "## üì§ Enviando os Dados Processados para o Elasticsearch\n",
    "Os tweets processados ser√£o armazenados no Elasticsearch para visualiza√ß√£o no Kibana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95291383",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install elasticsearch\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "for row in df.collect():\n",
    "    doc = {\n",
    "        \"user\": row.user,\n",
    "        \"text\": row.text,\n",
    "        \"keywords\": row.keywords,\n",
    "        \"created_at\": row.created_at\n",
    "    }\n",
    "    es.index(index=\"tweets\", body=doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
